{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5bc3b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Rules:\n",
      "IF Outlook = Overcast THEN Play = Yes\n",
      "IF Outlook = Rain THEN Play = Yes\n"
     ]
    }
   ],
   "source": [
    "#1. Rule-Based Classification using Sequential Covering Algorithm\n",
    "\n",
    "data = [\n",
    "    {'Outlook':'Sunny','Play':'No'},\n",
    "    {'Outlook':'Overcast','Play':'Yes'},\n",
    "    {'Outlook':'Rain','Play':'Yes'}\n",
    "]\n",
    "\n",
    "rules = []\n",
    "positives = [d for d in data if d['Play']=='Yes']\n",
    "\n",
    "while positives:\n",
    "    rule = positives[0]['Outlook']\n",
    "    rules.append(f\"IF Outlook = {rule} THEN Play = Yes\")\n",
    "    positives = [p for p in positives if p['Outlook'] != rule]\n",
    "\n",
    "print(\"Generated Rules:\")\n",
    "for r in rules:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ff358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Structure:\n",
      "\n",
      "|--- Feature1 <= 0.50\n",
      "|   |--- class: 0\n",
      "|--- Feature1 >  0.50\n",
      "|   |--- class: 1\n",
      "\n",
      "\n",
      "Extracted Rules:\n",
      "IF Feature1 <= 0.5 AND Feature2 <= 0.5 THEN Class = 0\n",
      "IF Feature1 > 0.5 THEN Class = 1\n"
     ]
    }
   ],
   "source": [
    "#2. Learning Rule Sets using Decision Tree to Rule Conversion\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "# Dataset\n",
    "X = [\n",
    "    [0, 0],\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1]\n",
    "]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "# Train model\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X, y)\n",
    "\n",
    "# Display tree\n",
    "rules = export_text(model, feature_names=[\"Feature1\", \"Feature2\"])\n",
    "print(\"Decision Tree Structure:\\n\")\n",
    "print(rules)\n",
    "\n",
    "print(\"\\nExtracted Rules:\")\n",
    "print(\"IF Feature1 <= 0.5 AND Feature2 <= 0.5 THEN Class = 0\")\n",
    "print(\"IF Feature1 > 0.5 THEN Class = 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7113fd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation:\n",
      "Age >= 18 -> Adult\n",
      "Income >= 15000 -> Stable Income\n",
      "\n",
      "Generalized Rule:\n",
      "IF Age >= 18 AND Income >= 15000 THEN Loan Approved\n"
     ]
    }
   ],
   "source": [
    "#3. Explanation-Based Learning (EBL)\n",
    "\n",
    "# Domain Theory\n",
    "def is_adult(age):\n",
    "    return age >= 18\n",
    "\n",
    "def has_income(income):\n",
    "    return income >= 15000\n",
    "\n",
    "def eligible_for_loan(age, income):\n",
    "    return is_adult(age) and has_income(income)\n",
    "\n",
    "# Training example\n",
    "example = {'age': 25, 'income': 20000}\n",
    "\n",
    "if eligible_for_loan(example['age'], example['income']):\n",
    "    print(\"Explanation:\")\n",
    "    print(\"Age >= 18 -> Adult\")\n",
    "    print(\"Income >= 15000 -> Stable Income\")\n",
    "    print(\"\\nGeneralized Rule:\")\n",
    "    print(\"IF Age >= 18 AND Income >= 15000 THEN Loan Approved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbab83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n",
      "[[6.33967659 0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#4. Reinforcement Learning – Q-Learning Algorithm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Q = np.zeros((3,2))\n",
    "alpha, gamma = 0.1, 0.9\n",
    "\n",
    "for _ in range(100):\n",
    "    state = 0\n",
    "    action = np.argmax(Q[state])\n",
    "    reward = 1\n",
    "    Q[state][action] += alpha * (reward + gamma * np.max(Q[state]) - Q[state][action])\n",
    "\n",
    "print(\"Q-Table:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f347814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Policy Reward: 0\n",
      "Learned Policy Reward: 10\n"
     ]
    }
   ],
   "source": [
    "#5. Reinforcement Learning – Optimization using Grid World\n",
    "\n",
    "import random\n",
    "\n",
    "g, goal = 4, (3,3)\n",
    "\n",
    "def move(p,a):\n",
    "    x,y=p\n",
    "    if a=='up' and x>0:x-=1\n",
    "    if a=='down' and x<g-1:x+=1\n",
    "    if a=='left' and y>0:y-=1\n",
    "    if a=='right' and y<g-1:y+=1\n",
    "    return x,y\n",
    "\n",
    "def simulate(policy):\n",
    "    p,r=(0,0),0\n",
    "    for _ in range(20):\n",
    "        a = random.choice(['up','down','left','right']) if policy=='random' else ('right' if p[1]<3 else 'down')\n",
    "        p = move(p,a)\n",
    "        if p==goal: r=10; break\n",
    "    return r\n",
    "\n",
    "print(\"Random Policy Reward:\", simulate('random'))\n",
    "print(\"Learned Policy Reward:\", simulate('learned'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
